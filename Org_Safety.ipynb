{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1abo6oM9-8XtWJV2FUuZi0i_sqQhpSVsI",
      "authorship_tag": "ABX9TyNNJx+vljC1kIjerKr9F/rO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jadhav7850/jango/blob/master/Org_Safety.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nwDk6RuyMnta"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import holidays\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import lit, min, max, pandas_udf, PandasUDFType\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import shutil\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "import logging\n",
        "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
        "\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from pyspark.sql.functions import col\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, date_add, next_day\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "import mlflow.sklearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1load the historical dataset\n",
        "#from pyspark.sql import SparkSession\n",
        "#from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType\n",
        "\n",
        "\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Define the schema for the historical dataset\n",
        "history_schema = StructType([\n",
        "    StructField('date', TimestampType()),\n",
        "    StructField('store', IntegerType()),\n",
        "    StructField('item', IntegerType()),\n",
        "    StructField('sales', IntegerType())\n",
        "])\n",
        "\n",
        "# Load the historical dataset\n",
        "history = spark.read.csv(\n",
        "    '/content/reduced_dataset (2).csv',\n",
        "    header=True,\n",
        "    schema=history_schema\n",
        ").cache()\n",
        "\n",
        "# Create a temporary view for the historical dataset\n",
        "history.createOrReplaceTempView('history_tmp')\n",
        "\n"
      ],
      "metadata": {
        "id": "7B2SnsMCNjO2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2generate holiday information for years in the historical dataset and\n",
        "# the following year should forecasts extend into that year\n",
        "\n",
        "\n",
        "# Generate holiday information for years in the historical dataset and the following year\n",
        "first_date, last_date = history.selectExpr(\"min(date)\", \"max(date)\").first()\n",
        "\n",
        "holidays_df = spark.createDataFrame(\n",
        "    [(date, holiday) for date, holiday in holidays.UnitedStates(years=range(first_date.year, last_date.year + 2)).items()],\n",
        "    ['date', 'holiday']\n",
        ").orderBy('date')\n",
        "\n",
        "holidays_pd = holidays_df.toPandas()\n",
        "holidays_broadcast = holidays_pd\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eR7XOPmgPBun"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate holiday information for years in the historical dataset and the following year\n",
        "first_date, last_date = (\n",
        "    history\n",
        "        .selectExpr(\"min(date)\", \"max(date)\")\n",
        "        .first()\n",
        ")\n",
        "\n",
        "holidays_df = spark.createDataFrame(\n",
        "    [(date, holiday) for date, holiday in holidays.UnitedStates(years=range(first_date.year, 2 + last_date.year)).items()],\n",
        "    ['date', 'holiday']\n",
        ").orderBy('date')\n",
        "\n",
        "holidays_pd = holidays_df.toPandas()\n",
        "holidays_broadcast = holidays_pd\n"
      ],
      "metadata": {
        "id": "5RdzbLgGQVZ3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(holidays_broadcast)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOaIlK1dSAT-",
        "outputId": "4320034c-0679-4258-a743-0a32f6c396ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          date                     holiday\n",
            "0   2013-01-01              New Year's Day\n",
            "1   2013-01-21  Martin Luther King Jr. Day\n",
            "2   2013-02-18       Washington's Birthday\n",
            "3   2013-05-27                Memorial Day\n",
            "4   2013-07-04            Independence Day\n",
            "..         ...                         ...\n",
            "60  2018-10-08                Columbus Day\n",
            "61  2018-11-11                Veterans Day\n",
            "62  2018-11-12     Veterans Day (Observed)\n",
            "63  2018-11-22                Thanksgiving\n",
            "64  2018-12-25               Christmas Day\n",
            "\n",
            "[65 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "def get_forecast(keys, history_pd):\n",
        "    # Read keys associated with grouped data\n",
        "    store = keys[0]\n",
        "    item = keys[1]\n",
        "    days_to_forecast = keys[2]\n",
        "\n",
        "    # Prepare grouped data for training\n",
        "    history_pd = history_pd.dropna()\n",
        "    history_pd.rename(columns={'date': 'ds', 'sales': 'y'}, inplace=True)\n",
        "\n",
        "    # Acquire holiday dataset\n",
        "    holidays_pd = holidays_df\n",
        "\n",
        "    # Instantiate and configure Prophet model\n",
        "    model = Prophet(\n",
        "        interval_width=0.95,\n",
        "        growth='linear',\n",
        "        daily_seasonality=False,\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=True,\n",
        "        seasonality_mode='multiplicative',\n",
        "        holidays=holidays_pd\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(history_pd[['ds', 'y']])\n",
        "\n",
        "    # Save fitted model for later utilization (during evaluation)\n",
        "    model_path = '/tmp/forecast/{0}_{1}'.format(store, item)\n",
        "    shutil.rmtree(model_path, ignore_errors=True)\n",
        "    mlflow.sklearn.save_model(model, model_path)\n",
        "\n",
        "    # Make forecast dataset\n",
        "    future_pd = model.make_future_dataframe(\n",
        "        periods=days_to_forecast,\n",
        "        freq='d',\n",
        "        include_history=True\n",
        "    )\n",
        "\n",
        "    # Generate forecast\n",
        "    forecast_pd = model.predict(future_pd)\n",
        "\n",
        "    # Merge history and forecast datasets\n",
        "    forecast_pd = forecast_pd.merge(history_pd, on='ds', how='left')\n",
        "\n",
        "    # Assign store and item to results\n",
        "    forecast_pd['store'] = store\n",
        "    forecast_pd['item'] = item\n",
        "\n",
        "    # Return results\n",
        "    forecast_pd.rename(\n",
        "        columns={\n",
        "            'ds': 'date', 'y': 'sales',\n",
        "            'yhat': 'sales_pred_mean', 'yhat_lower': 'sales_pred_lower', 'yhat_upper': 'sales_pred_upper'\n",
        "        },\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    return forecast_pd[['store', 'item', 'date', 'sales', 'sales_pred_mean', 'sales_pred_lower', 'sales_pred_upper']]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4_tBQK20ZO34"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Identical to the above code\n",
        "def get_forecast(keys, history_pd):\n",
        "    # Read keys associated with grouped data\n",
        "    store = keys[0]\n",
        "    item = keys[1]\n",
        "    days_to_forecast = keys[2]\n",
        "\n",
        "    # Prepare grouped data for training\n",
        "    history_pd = history_pd.dropna()\n",
        "    history_pd.rename(columns={'date': 'ds', 'sales': 'y'}, inplace=True)\n",
        "\n",
        "    # Acquire holiday dataset\n",
        "    holidays_pd = holidays_broadcast\n",
        "\n",
        "    # Instantiate and configure Prophet model\n",
        "    model = Prophet(\n",
        "        interval_width=0.95,\n",
        "        growth='linear',\n",
        "        daily_seasonality=False,\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=True,\n",
        "        seasonality_mode='multiplicative',\n",
        "        holidays=holidays_pd\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(history_pd[['ds', 'y']])\n",
        "\n",
        "    # Save fitted model for later utilization (during evaluation)\n",
        "    model_path = '/tmp/forecast/{0}_{1}'.format(store, item)\n",
        "    shutil.rmtree(model_path, ignore_errors=True)\n",
        "    mlflow.sklearn.save_model(model, model_path)\n",
        "\n",
        "    # Make forecast dataset\n",
        "    future_pd = model.make_future_dataframe(\n",
        "        periods=days_to_forecast,\n",
        "        freq='d',\n",
        "        include_history=True\n",
        "    )\n",
        "\n",
        "    # Generate forecast\n",
        "    forecast_pd = model.predict(future_pd)\n",
        "\n",
        "    # Merge history and forecast datasets\n",
        "    forecast_pd = forecast_pd.merge(history_pd, on='ds', how='left')\n",
        "\n",
        "    # Assign store and item to results\n",
        "    forecast_pd['store'] = store\n",
        "    forecast_pd['item'] = item\n",
        "\n",
        "    # Return results\n",
        "    forecast_pd.rename(\n",
        "        columns={\n",
        "            'ds': 'date', 'y': 'sales',\n",
        "            'yhat': 'sales_pred_mean', 'yhat_lower': 'sales_pred_lower', 'yhat_upper': 'sales_pred_upper'\n",
        "        },\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    return forecast_pd[['store', 'item', 'date', 'sales', 'sales_pred_mean', 'sales_pred_lower', 'sales_pred_upper']]\n",
        "\n",
        "import mlflow.sklearn\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x4I5b-lqSO2K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4Use of get_forecast function.(Code use)\n",
        "\n",
        "# Define the historical sales data as a pandas DataFrame\n",
        "history_data = {\n",
        "    'date': ['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04'],\n",
        "    'sales': [10, 15, 12, 8]\n",
        "}\n",
        "history_pd = pd.DataFrame(history_data)#Datetime format\n",
        "\n",
        "# Convert the 'date' column to datetime type(format that represents date)\n",
        "history_pd['date'] = pd.to_datetime(history_pd['date'])\n",
        "\n",
        "# Define the keys variable(info)\n",
        "keys = ['store1', 'item1', 7]\n",
        "\n",
        "# Define the holiday dataset DataFrame\n",
        "holidays_data = {\n",
        "    'ds': [\n",
        "        '2013-01-01', '2013-01-21', '2013-02-18', '2013-05-27', '2013-07-04',\n",
        "\n",
        "    ],\n",
        "    'holiday': [\n",
        "        \"New Year's Day\", \"Martin Luther King Jr. Day\", \"Washington's Birthday\",\n",
        "        \"Memorial Day\", \"Independence Day\",\n",
        "\n",
        "    ]\n",
        "}\n",
        "holidays_pd = pd.DataFrame(holidays_data)\n",
        "\n",
        "# Convert the 'ds' column to datetime type\n",
        "holidays_pd['ds'] = pd.to_datetime(holidays_pd['ds'])#convert ds to datetime\n",
        "\n",
        "# Assign the holiday dataset to holidays_broadcast\n",
        "holidays_broadcast = holidays_pd\n",
        "\n",
        "# Call the get_forecast function\n",
        "#generates forecasts ,returns  DataFrame with forecasted sales data.\n",
        "result = get_forecast(keys, history_pd)\n",
        "\n",
        "# Merge history and forecast datasets\n",
        "result = pd.concat([result, history_pd], axis=0, ignore_index=True)\n",
        "\n",
        "# Print the result\n",
        "#print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On-LVCy8Uw9S",
        "outputId": "7505a044-5768-4d25-c26e-17cf6480a0e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prophet:n_changepoints greater than number of observations. Using 2.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp7du4xoub/mgqnkycp.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp7du4xoub/vf_lsi95.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=80045', 'data', 'file=/tmp/tmp7du4xoub/mgqnkycp.json', 'init=/tmp/tmp7du4xoub/vf_lsi95.json', 'output', 'file=/tmp/tmp7du4xoub/prophet_modelpdkhno5e/prophet_model-20230710040138.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "04:01:38 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "04:01:38 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_pd = get_forecast(keys, history_pd)\n",
        "print(forecast_pd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFxFX_V_V8zJ",
        "outputId": "e0277f64-7efc-4742-ff4e-52db5fddda2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prophet:n_changepoints greater than number of observations. Using 2.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp7du4xoub/k9arqs59.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp7du4xoub/f48kvepk.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=99167', 'data', 'file=/tmp/tmp7du4xoub/k9arqs59.json', 'init=/tmp/tmp7du4xoub/f48kvepk.json', 'output', 'file=/tmp/tmp7du4xoub/prophet_modelrc4ifrem/prophet_model-20230710040142.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "04:01:42 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "04:01:43 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     store   item       date  sales  sales_pred_mean  sales_pred_lower  \\\n",
            "0   store1  item1 2013-01-01   10.0        10.000027         10.000027   \n",
            "1   store1  item1 2013-01-02   15.0        15.000065         15.000065   \n",
            "2   store1  item1 2013-01-03   12.0        12.000109         12.000109   \n",
            "3   store1  item1 2013-01-04    8.0         8.000131          8.000131   \n",
            "4   store1  item1 2013-01-05    NaN         0.838181          0.838181   \n",
            "5   store1  item1 2013-01-06    NaN       -22.360321        -22.360324   \n",
            "6   store1  item1 2013-01-07    NaN       -13.610753        -13.610755   \n",
            "7   store1  item1 2013-01-08    NaN         9.921528          9.921526   \n",
            "8   store1  item1 2013-01-09    NaN       -12.091487        -12.091489   \n",
            "9   store1  item1 2013-01-10    NaN       -22.504478        -22.504482   \n",
            "10  store1  item1 2013-01-11    NaN       -50.393172        -50.393182   \n",
            "\n",
            "    sales_pred_upper  \n",
            "0          10.000027  \n",
            "1          15.000065  \n",
            "2          12.000109  \n",
            "3           8.000131  \n",
            "4           0.838182  \n",
            "5         -22.360317  \n",
            "6         -13.610750  \n",
            "7           9.921530  \n",
            "8         -12.091484  \n",
            "9         -22.504473  \n",
            "10        -50.393160  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Evaluate the forecast performance\n",
        "#forecast_pd:-DataFrame containing the forecasted sales data.\n",
        "def evaluate_forecast(keys, forecast_pd):\n",
        "    # Read keys associated with grouped data\n",
        "    forecast_date = keys[0]\n",
        "    store = int(keys[1])\n",
        "    item = int(keys[2])\n",
        "\n",
        "    # Calculate MSE & RMSE\n",
        "    mse = mean_squared_error(forecast_pd['sales'], forecast_pd['sales_pred_mean'])\n",
        "    rmse = sqrt(mse)\n",
        "\n",
        "    # Calculate MAE\n",
        "    mae = mean_absolute_error(forecast_pd['sales'], forecast_pd['sales_pred_mean'])\n",
        "\n",
        "    # Calculate MAPE\n",
        "    mape = np.mean(np.abs((forecast_pd['sales'] - forecast_pd['sales_pred_mean']) / forecast_pd['sales'])) * 100\n",
        "\n",
        "    # Assemble result set\n",
        "    results = {\n",
        "        'forecast_date': [forecast_date],\n",
        "        'store': [store],\n",
        "        'item': [item],\n",
        "        'mse': [mse],\n",
        "        'rmse': [rmse],\n",
        "        'mae': [mae],\n",
        "        'mape': [mape]\n",
        "    }\n",
        "\n",
        "    print(results)  # Print the results\n",
        "    return pd.DataFrame(data=results)\n",
        "    #evaluation_result = pd.DataFrame(data=results)\n",
        "    #print(evaluation_result)  # Print the results\n",
        "    #return evaluation_result\n",
        "\n",
        "\n",
        "    #The goal of this code is to evaluate\n",
        "    #the forecast performance and provide a summary of the evaluation metrics for a given forecast.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s4geQf4xTHTK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = ['forecast_date', 1, 2]  # Replace 'store' with an actual store value and 'item' with an actual item value\n",
        "forecast_data = {\n",
        "    'sales': [10, 15, 12, 8],#Actual sales\n",
        "    'sales_pred_mean': [11, 14, 10, 9]#Forecasted sales values\n",
        "}\n",
        "forecast_pd = pd.DataFrame(forecast_data)\n",
        "\n",
        "# Call the evaluate_forecast function\n",
        "evaluation_result = evaluate_forecast(keys, forecast_pd)\n",
        "\n",
        "# Print the evaluation result\n",
        "print(evaluation_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbJah344BiS6",
        "outputId": "63dd666b-289c-468c-bde6-2b57844c0a26"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'forecast_date': ['forecast_date'], 'store': [1], 'item': [2], 'mse': [1.75], 'rmse': [1.3228756555322954], 'mae': [1.25], 'mape': [11.458333333333334]}\n",
            "   forecast_date  store  item   mse      rmse   mae       mape\n",
            "0  forecast_date      1     2  1.75  1.322876  1.25  11.458333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forecastresult=evaluate_forecast(keys, forecast_pd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aZ49zN5XyuA",
        "outputId": "e80b2a12-43b6-4204-98b0-6609ff0c7c4e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'forecast_date': ['forecast_date'], 'store': [1], 'item': [2], 'mse': [1.75], 'rmse': [1.3228756555322954], 'mae': [1.25], 'mape': [11.458333333333334]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(forecastresult)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChJ0lfloX6Fy",
        "outputId": "f65f252e-69e6-4ff3-dc96-f2dd7ec8eb40"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   forecast_date  store  item   mse      rmse   mae       mape\n",
            "0  forecast_date      1     2  1.75  1.322876  1.25  11.458333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6define function to generate forecast cross-validation evaluation metrics\n",
        "def evaluate_forecast_cv(keys, forecast_pd):\n",
        "    # read keys associated with grouped data (Extracts)\n",
        "    forecast_date = keys[0]\n",
        "    store = int(keys[1])\n",
        "    item = int(keys[2])\n",
        "    days_to_forecast = int(keys[3])\n",
        "\n",
        "    # retrieve trained model\n",
        "    model_path = '/dbfs/tmp/forecast/{0}_{1}'.format(store, item)\n",
        "    model = mlflow.sklearn.load_model(model_path)\n",
        "\n",
        "    # calculate cv performance metrics\n",
        "    crossval_pd = cross_validation(\n",
        "        model,\n",
        "        initial='1095 days',\n",
        "        horizon='{0} days'.format(days_to_forecast)\n",
        "    )\n",
        "    perf_pd = performance_metrics(\n",
        "        crossval_pd,\n",
        "        metrics=['mse', 'rmse', 'mae', 'mape', 'mdape', 'coverage']\n",
        "    )\n",
        "\n",
        "    # prepare results\n",
        "    result = perf_pd.copy()\n",
        "    result['forecast_date'] = forecast_date\n",
        "    result['store'] = store\n",
        "    result['item'] = item\n",
        "    result['horizon'] = result['horizon'].apply(lambda h: h.days)\n",
        "\n",
        "    # return metrics\n",
        "    return result[['forecast_date', 'store', 'item', 'horizon', 'mse', 'rmse', 'mae', 'mape', 'mdape', 'coverage']]\n",
        "\n"
      ],
      "metadata": {
        "id": "O9_WbMyRTvip"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate_forecast_cv(keys, forecast_pd)"
      ],
      "metadata": {
        "id": "tob7KqS4ufgZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modified version of evaluate_forecast_cv\n",
        "import mlflow.sklearn\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_forecast_cv(keys, forecast_pd):\n",
        "    # Read keys associated with grouped data\n",
        "    forecast_date = keys[0]\n",
        "    store = keys[1]\n",
        "    item = keys[2]\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = forecast_pd['mse'].values[0]\n",
        "    rmse = forecast_pd['rmse'].values[0]\n",
        "    mae = forecast_pd['mae'].values[0]\n",
        "    mape = forecast_pd['mape'].values[0]\n",
        "\n",
        "    # Assemble result set\n",
        "    results = {\n",
        "        'forecast_date': [forecast_date],\n",
        "        'store': [store],\n",
        "        'item': [item],\n",
        "        'mse': [mse],\n",
        "        'rmse': [rmse],\n",
        "        'mae': [mae],\n",
        "        'mape': [mape]\n",
        "    }\n",
        "    return pd.DataFrame(data=results)\n"
      ],
      "metadata": {
        "id": "uCfBIIM3Cdo6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "forecast_date = '2017-12-01'\n",
        "spark.conf.set('spark.sql.shuffle.partitions', 500 )\n",
        "\n",
        "# generate forecast for this data\n",
        "forecasts = (\n",
        "  history\n",
        "  .where(history.date < forecast_date) # limit training data to prior to our forecast date\n",
        "  .groupBy('store', 'item', lit(30).alias('days_to_forecast'))\n",
        "    .applyInPandas(get_forecast, \"store integer, item integer, date timestamp, sales float, sales_pred_mean float, sales_pred_lower float, sales_pred_upper float\")\n",
        "    .withColumn('forecast_date', lit(forecast_date).cast(TimestampType()))\n",
        "    ).cache()\n",
        "\n",
        "#calculates evaluation metrics for the generated forecasts\n",
        "forecast_evals = (\n",
        "  forecasts\n",
        "    .select('forecast_date', 'store', 'item', 'sales', 'sales_pred_mean')\n",
        "    .where(forecasts.date < forecasts.forecast_date)\n",
        "    .groupBy('forecast_date', 'store', 'item')\n",
        "    .applyInPandas(evaluate_forecast, \"forecast_date timestamp, store integer, item integer, mse float, rmse float, mae float, mape float\")\n",
        "    )\n",
        "\n",
        "#calculates cross-validation evaluation metrics for the forecasts\n",
        "forecast_evals_cv = (\n",
        "  forecasts\n",
        "    .select('forecast_date', 'store', 'item', 'sales', 'sales_pred_mean')\n",
        "    .where(forecasts.date < forecasts.forecast_date)\n",
        "    .groupBy('forecast_date', 'store', 'item', lit(30).alias('days_to_forecast'))\n",
        "    .applyInPandas(evaluate_forecast_cv, \"forecast_date timestamp, store integer, item integer, horizon integer, mse float, rmse float, mae float, mape float, mdape float, coverage float\")\n",
        "    )\n",
        "\n",
        "forecasts.createOrReplaceTempView('forecasts_tmp')\n",
        "forecast_evals.createOrReplaceTempView('forecast_evals_tmp')\n",
        "forecast_evals_cv.createOrReplaceTempView('forecast_evals_cv_tmp')\n"
      ],
      "metadata": {
        "id": "6rbQuPYDVU9Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(forecasts)\n",
        "print(forecast_evals)\n",
        "print(forecast_evals_cv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCHhQuh22cgE",
        "outputId": "12066636-b00c-43ec-f688-76efe5900d88"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[store: int, item: int, date: timestamp, sales: float, sales_pred_mean: float, sales_pred_lower: float, sales_pred_upper: float, forecast_date: timestamp]\n",
            "DataFrame[forecast_date: timestamp, store: int, item: int, mse: float, rmse: float, mae: float, mape: float]\n",
            "DataFrame[forecast_date: timestamp, store: int, item: int, horizon: int, mse: float, rmse: float, mae: float, mape: float, mdape: float, coverage: float]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8Save the forecast data as a CSV file using Spark.\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Forecast Persistence\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set the default database\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS solacc_safety_stock\")\n",
        "spark.sql(\"USE solacc_safety_stock\")\n",
        "\n",
        "# Drop the existing table if it exists\n",
        "spark.sql(\"DROP TABLE IF EXISTS forecasts\")\n",
        "\n",
        "# Create the forecasts table\n",
        "forecasts_df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        a.forecast_date,\n",
        "        a.store,\n",
        "        a.item,\n",
        "        a.date,\n",
        "        b.sales,\n",
        "        a.sales_pred_mean,\n",
        "        a.sales_pred_lower,\n",
        "        a.sales_pred_upper\n",
        "    FROM forecasts_tmp a\n",
        "    INNER JOIN history_tmp b\n",
        "        ON a.store=b.store AND a.item=b.item AND a.date=b.date\n",
        "\"\"\")\n",
        "\n",
        "# Save the forecasts table as a CSV file\n",
        "forecasts_df.write.mode(\"overwrite\").csv(\"/content/new_path_to_save_forecast_evals.csv\", header=True)\n",
        "\n",
        "# Stop the Spark session\n",
        " #spark.stop()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O1O3w6Z4cZQ7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9Code extension to read Spark DataFrame, convert to Pandas DataFrame, and save as a CSV file\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Forecast Persistence\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set the default database\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS solacc_safety_stock\")\n",
        "spark.sql(\"USE solacc_safety_stock\")\n",
        "\n",
        "# Drop the existing table if it exists\n",
        "spark.sql(\"DROP TABLE IF EXISTS forecast_evals\")\n",
        "\n",
        "# Read the forecast_evals_tmp data from a local CSV file\n",
        "forecast_evals_tmp_df = spark.read.csv(\"/content/new_path_to_save_forecast_evals.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Create the forecast_evals table\n",
        "forecast_evals_tmp_df.createOrReplaceTempView(\"forecast_evals_tmp\")\n",
        "forecast_evals_df = spark.sql(\"SELECT * FROM forecast_evals_tmp\")\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "forecast_evals_pandas_df = forecast_evals_df.toPandas()\n",
        "\n",
        "# Save the forecast_evals Pandas DataFrame as a CSV file locally\n",
        "forecast_evals_pandas_df.to_csv(\"path_to_save_forecast_evals.csv\", index=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SEJGZvUSL9gY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4152f6b2-d1ae-463c-d10e-a116a8e499b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10forecast_evals_cv_tmp_df\n",
        "# Set the default database\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS solacc_safety_stock\")\n",
        "spark.sql(\"USE solacc_safety_stock\")\n",
        "\n",
        "# Drop the existing table if it exists\n",
        "spark.sql(\"DROP TABLE IF EXISTS forecast_evals_cv\")\n",
        "\n",
        "# Read the forecast_evals_cv_tmp data from a local CSV file\n",
        "forecast_evals_cv_tmp_df = spark.read.csv(\"/content/new_path_to_save_forecast_evals.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Create the forecast_evals_cv table\n",
        "forecast_evals_cv_tmp_df.createOrReplaceTempView(\"forecast_evals_cv_tmp\")\n",
        "forecast_evals_cv_df = spark.sql(\"SELECT * FROM forecast_evals_cv_tmp\")\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "forecast_evals_cv_pandas_df = forecast_evals_cv_df.toPandas()\n",
        "\n",
        "# Save the forecast_evals_cv Pandas DataFrame as a CSV file locally\n",
        "forecast_evals_cv_pandas_df.to_csv(\"path_to_save_forecast_evals_cv.csv\", index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ywlfnBF2ZyGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04072b14-59e5-491b-a70a-a5a0411ea0ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11forecasts_df\n",
        "# extract a subset of data from the forecasts_df\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Load data from a CSV file into a Spark DataFrame\n",
        "forecasts_df = spark.read.csv('/content/new_path_to_save_forecast_evals.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Register the forecasts_df DataFrame as a temporary table\n",
        "forecasts_df.createOrReplaceTempView(\"forecasts_table\")\n",
        "\n",
        "# Execute the SQL query to filter the data\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  date,\n",
        "  sales,\n",
        "  sales_pred_mean\n",
        "FROM forecasts_table\n",
        "WHERE\n",
        "  store = 1 AND\n",
        "  item = 1 AND\n",
        "  YEAR(date) = 2017\n",
        "\"\"\"\n",
        "\n",
        "# Run the query and get the result as a DataFrame\n",
        "result_df = spark.sql(query)\n",
        "\n",
        "# Convert the result DataFrame to a Pandas DataFrame\n",
        "pandas_df = result_df.toPandas()\n",
        "\n",
        "# Display the result\n",
        "print(pandas_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7audJvlqaSKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6909dec-e429-4bc4-d757-fd882e53ab4c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          date  sales  sales_pred_mean\n",
            "0   2017-01-01     19        20.230690\n",
            "1   2017-01-02     15        11.159161\n",
            "2   2017-01-03     10        14.131730\n",
            "3   2017-01-04     16        14.736224\n",
            "4   2017-01-05     14        15.449884\n",
            "..         ...    ...              ...\n",
            "359 2017-12-26     16        14.065387\n",
            "360 2017-12-27     14        14.827838\n",
            "361 2017-12-28     19        15.707889\n",
            "362 2017-12-29     15        17.629807\n",
            "363 2017-12-30     27        19.861452\n",
            "\n",
            "[364 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12percent days below and above forecast by model\n",
        "# Restart the SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Assuming you have loaded the data into a Spark DataFrame called `forecasts_df`\n",
        "\n",
        "# Register the forecasts_df DataFrame as a temporary table\n",
        "forecasts_df.createOrReplaceTempView(\"forecasts\")\n",
        "\n",
        "# Execute the SQL query to calculate percent days below and above the forecast by model\n",
        "result_df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        store,\n",
        "        item,\n",
        "        COUNT(*) as dates_total,\n",
        "        SUM(CASE WHEN sales < sales_pred_mean THEN 1 ELSE 0 END) as dates_below_forecast,\n",
        "        SUM(CASE WHEN sales > sales_pred_mean THEN 1 ELSE 0 END) as dates_above_forecast,\n",
        "        FORMAT_NUMBER(SUM(CASE WHEN sales < sales_pred_mean THEN 1 ELSE 0 END) / COUNT(*), '##.#%') as dates_below_forecast_pct,\n",
        "        FORMAT_NUMBER(SUM(CASE WHEN sales > sales_pred_mean THEN 1 ELSE 0 END) / COUNT(*), '##.#%') as dates_above_forecast_pct\n",
        "    FROM forecasts\n",
        "    GROUP BY store, item\n",
        "    ORDER BY store, item\n",
        "\"\"\")\n",
        "\n",
        "# Convert the percentages to double type\n",
        "result_df = result_df.withColumn(\"dates_below_forecast_pct\", col(\"dates_below_forecast_pct\").cast(DoubleType()))\n",
        "result_df = result_df.withColumn(\"dates_above_forecast_pct\", col(\"dates_above_forecast_pct\").cast(DoubleType()))\n",
        "\n",
        "# Show the result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "id": "7CSO8AAwcXyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5615de98-8689-4494-a321-21dd4941a6e7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-----------+--------------------+--------------------+------------------------+------------------------+\n",
            "|store|item|dates_total|dates_below_forecast|dates_above_forecast|dates_below_forecast_pct|dates_above_forecast_pct|\n",
            "+-----+----+-----------+--------------------+--------------------+------------------------+------------------------+\n",
            "|    1|   1|       1825|                 937|                 888|                    null|                    null|\n",
            "|    2|   1|       1825|                 955|                 870|                    null|                    null|\n",
            "|    3|   1|       1825|                 944|                 881|                    null|                    null|\n",
            "|    4|   1|       1825|                 939|                 886|                    null|                    null|\n",
            "|    5|   1|       1825|                 961|                 864|                    null|                    null|\n",
            "|    6|   1|        870|                 447|                 423|                    null|                    null|\n",
            "+-----+----+-----------+--------------------+--------------------+------------------------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13Calculates the cycle associated with each date in the \"forecasts_df\" DataFrame\n",
        "# Restart the SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Assuming you have loaded the data into a Spark DataFrame called `forecasts_df`\n",
        "\n",
        "# Calculate the cycle with which each date is associated\n",
        "result_df = forecasts_df.select(\n",
        "    col(\"store\"),\n",
        "    col(\"item\"),\n",
        "    col(\"date\"),\n",
        "    next_day(date_add(col(\"date\"), -7), \"Thursday\").alias(\"cycle_start\"),\n",
        "    next_day(date_add(col(\"date\"), -1), \"Wednesday\").alias(\"cycle_end\"),\n",
        "    date_add(next_day(date_add(col(\"date\"), -7), \"Thursday\"), -4).alias(\"order_date\")\n",
        ").orderBy(\"store\", \"item\", \"date\")\n",
        "\n",
        "# Show the result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eazKYSObRVXO",
        "outputId": "b2937448-7358-42e2-a11a-9a3bd84aefac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-------------------+-----------+----------+----------+\n",
            "|store|item|               date|cycle_start| cycle_end|order_date|\n",
            "+-----+----+-------------------+-----------+----------+----------+\n",
            "|    1|   1|2013-01-01 00:00:00| 2012-12-27|2013-01-02|2012-12-23|\n",
            "|    1|   1|2013-01-02 00:00:00| 2012-12-27|2013-01-02|2012-12-23|\n",
            "|    1|   1|2013-01-03 00:00:00| 2013-01-03|2013-01-09|2012-12-30|\n",
            "|    1|   1|2013-01-04 00:00:00| 2013-01-03|2013-01-09|2012-12-30|\n",
            "|    1|   1|2013-01-05 00:00:00| 2013-01-03|2013-01-09|2012-12-30|\n",
            "|    1|   1|2013-01-06 00:00:00| 2013-01-03|2013-01-09|2012-12-30|\n",
            "|    1|   1|2013-01-07 00:00:00| 2013-01-03|2013-01-09|2012-12-30|\n",
            "|    1|   1|2013-01-08 00:00:00| 2013-01-03|2013-01-09|2012-12-30|\n",
            "|    1|   1|2013-01-09 00:00:00| 2013-01-03|2013-01-09|2012-12-30|\n",
            "|    1|   1|2013-01-10 00:00:00| 2013-01-10|2013-01-16|2013-01-06|\n",
            "|    1|   1|2013-01-11 00:00:00| 2013-01-10|2013-01-16|2013-01-06|\n",
            "|    1|   1|2013-01-12 00:00:00| 2013-01-10|2013-01-16|2013-01-06|\n",
            "|    1|   1|2013-01-13 00:00:00| 2013-01-10|2013-01-16|2013-01-06|\n",
            "|    1|   1|2013-01-14 00:00:00| 2013-01-10|2013-01-16|2013-01-06|\n",
            "|    1|   1|2013-01-15 00:00:00| 2013-01-10|2013-01-16|2013-01-06|\n",
            "|    1|   1|2013-01-16 00:00:00| 2013-01-10|2013-01-16|2013-01-06|\n",
            "|    1|   1|2013-01-17 00:00:00| 2013-01-17|2013-01-23|2013-01-13|\n",
            "|    1|   1|2013-01-18 00:00:00| 2013-01-17|2013-01-23|2013-01-13|\n",
            "|    1|   1|2013-01-19 00:00:00| 2013-01-17|2013-01-23|2013-01-13|\n",
            "|    1|   1|2013-01-20 00:00:00| 2013-01-17|2013-01-23|2013-01-13|\n",
            "+-----+----+-------------------+-----------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14Calculates the \"cycle_stock\" for each combination of store, item, cycle_start, and cycle\n",
        "\n",
        "\n",
        "# Restart the SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Assuming you have loaded the data into a Spark DataFrame called `forecasts_df`\n",
        "\n",
        "# Calculate cycle_stock\n",
        "cycles_df = forecasts_df.select(\n",
        "    col(\"store\"),\n",
        "    col(\"item\"),\n",
        "    next_day(date_add(col(\"date\"), -7), \"Thursday\").alias(\"cycle_start\"),\n",
        "    next_day(date_add(col(\"date\"), -1), \"Wednesday\").alias(\"cycle_end\"),\n",
        "    col(\"sales_pred_mean\")\n",
        ").distinct().where((F.year(col(\"cycle_start\")) >= 2013) & (F.year(col(\"cycle_end\")) <= 2017))\n",
        "\n",
        "window_spec = Window.partitionBy(\"store\", \"item\", \"cycle_start\", \"cycle_end\")\n",
        "\n",
        "result_df = cycles_df.withColumn(\"cycle_stock\", F.sum(col(\"sales_pred_mean\")).over(window_spec)) \\\n",
        "    .orderBy(\"store\", \"item\", \"cycle_start\")\n",
        "\n",
        "# Show the result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-7MtVdkW7FQ",
        "outputId": "039a7df6-a237-4755-86ab-c9de42c13332"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-----------+----------+---------------+-----------+\n",
            "|store|item|cycle_start| cycle_end|sales_pred_mean|cycle_stock|\n",
            "+-----+----+-----------+----------+---------------+-----------+\n",
            "|    1|   1| 2013-01-03|2013-01-09|      10.236608| 79.0632156|\n",
            "|    1|   1| 2013-01-03|2013-01-09|      7.7797976| 79.0632156|\n",
            "|    1|   1| 2013-01-03|2013-01-09|      13.701619| 79.0632156|\n",
            "|    1|   1| 2013-01-03|2013-01-09|      12.240011| 79.0632156|\n",
            "|    1|   1| 2013-01-03|2013-01-09|      10.992496| 79.0632156|\n",
            "|    1|   1| 2013-01-03|2013-01-09|      14.268439| 79.0632156|\n",
            "|    1|   1| 2013-01-03|2013-01-09|       9.844245| 79.0632156|\n",
            "|    1|   1| 2013-01-10|2013-01-16|      7.4127755|  76.753627|\n",
            "|    1|   1| 2013-01-10|2013-01-16|      13.362855|  76.753627|\n",
            "|    1|   1| 2013-01-10|2013-01-16|      10.716124|  76.753627|\n",
            "|    1|   1| 2013-01-10|2013-01-16|      9.9207535|  76.753627|\n",
            "|    1|   1| 2013-01-10|2013-01-16|       9.498183|  76.753627|\n",
            "|    1|   1| 2013-01-10|2013-01-16|      11.925282|  76.753627|\n",
            "|    1|   1| 2013-01-10|2013-01-16|      13.917654|  76.753627|\n",
            "|    1|   1| 2013-01-17|2013-01-23|        7.49289| 76.3240495|\n",
            "|    1|   1| 2013-01-17|2013-01-23|      11.708793| 76.3240495|\n",
            "|    1|   1| 2013-01-17|2013-01-23|       10.06508| 76.3240495|\n",
            "|    1|   1| 2013-01-17|2013-01-23|      13.212762| 76.3240495|\n",
            "|    1|   1| 2013-01-17|2013-01-23|      10.443361| 76.3240495|\n",
            "|    1|   1| 2013-01-17|2013-01-23|     13.8391485| 76.3240495|\n",
            "+-----+----+-----------+----------+---------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15calculates various stock-related metrics for each cycle\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Assuming you have loaded the data into a Spark DataFrame called 'forecasts_df'\n",
        "\n",
        "# Calculate cycle stock\n",
        "cycles_df = forecasts_df.groupby('store', 'item', F.window('date', '7 days', '1 day')).agg(\n",
        "    F.sum('sales_pred_mean').alias('cycle_stock'),\n",
        "    F.sum('sales').alias('cycle_sales'),\n",
        "    F.stddev('sales').alias('cycle_stddev')\n",
        ")\n",
        "\n",
        "# Filter cycles within the desired date range\n",
        "cycles_df = cycles_df.filter((F.year(cycles_df['window']['start']) >= 2013) & (F.year(cycles_df['window']['end']) <= 2017))\n",
        "\n",
        "# Calculate cycle days and z-score\n",
        "cycles_df = cycles_df.withColumn('cycle_days', F.datediff(cycles_df['window']['end'], cycles_df['window']['start']) + 1)\n",
        "cycles_df = cycles_df.withColumn('zscore', F.lit(1.6449))  # z-score for 95% SLE\n",
        "\n",
        "# Calculate safety stock and required stock\n",
        "cycles_df = cycles_df.withColumn('safety_stock', F.col('zscore') * F.sqrt('cycle_days') * F.col('cycle_stddev'))\n",
        "cycles_df = cycles_df.withColumn('required_stock', F.col('cycle_stock') + F.col('safety_stock'))\n",
        "\n",
        "# Sort the results\n",
        "window_spec = Window.partitionBy('store', 'item').orderBy('window')\n",
        "cycles_df = cycles_df.withColumn('cycle_start', F.col('window')['start'].cast('date'))\n",
        "cycles_df = cycles_df.orderBy('store', 'item', 'cycle_start').drop('window')\n",
        "\n",
        "# Display the result\n",
        "cycles_df.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfO0pEEDXS7f",
        "outputId": "bca5a106-deec-4cd8-e4f4-1e4bcd1611dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-----------------+-----------+------------------+----------+------+------------------+------------------+-----------+\n",
            "|store|item|      cycle_stock|cycle_sales|      cycle_stddev|cycle_days|zscore|      safety_stock|    required_stock|cycle_start|\n",
            "+-----+----+-----------------+-----------+------------------+----------+------+------------------+------------------+-----------+\n",
            "|    1|   1|       79.9960246|         83|1.5735915849388862|         8|1.6449| 7.321103026964487| 87.31712762696449| 2013-01-01|\n",
            "|    1|   1|       79.2866606|         79|1.7994708216848745|         8|1.6449| 8.372001608081204| 87.65866220808121| 2013-01-02|\n",
            "|    1|   1|       79.0632156|         80|1.8126539343499315|         8|1.6449|  8.43333577315982| 87.49655137315983| 2013-01-03|\n",
            "|    1|   1|       78.7868436|         75|1.6035674514745462|         8|1.6449| 7.460565139834534| 86.24740873983453| 2013-01-04|\n",
            "|    1|   1|78.47211460000001|         71| 1.345185418269098|         8|1.6449| 6.258447955478143| 84.73056255547816| 2013-01-05|\n",
            "|    1|   1|       78.1333506|         68| 1.799470821684875|         8|1.6449| 8.372001608081208|  86.5053522080812| 2013-01-06|\n",
            "|    1|   1|       77.7825656|         66| 1.511857892036909|         8|1.6449| 7.033888269147951| 84.81645386914795| 2013-01-07|\n",
            "|    1|   1|       77.4155435|         68|1.7994708216848752|         8|1.6449| 8.372001608081208|  85.7875451080812| 2013-01-08|\n",
            "|    1|   1|       77.0694815|         64|2.5448360411214073|         8|1.6449|11.839798218357803|  88.9092797183578| 2013-01-09|\n",
            "|    1|   1|76.75362700000001|         59|  2.29906813420444|         8|1.6449|10.696368001469338| 87.44999500146935| 2013-01-10|\n",
            "|    1|   1|        76.480864|         66| 3.690399384761441|         8|1.6449|17.169508508482625| 93.65037250848262| 2013-01-11|\n",
            "|    1|   1|        76.264375|         64|3.8047589248453675|         8|1.6449|17.701563956086723| 93.96593895608672| 2013-01-12|\n",
            "|    1|   1|76.11428199999999|         75|4.8892496259407645|         8|1.6449|22.747135011814443| 98.86141701181444| 2013-01-13|\n",
            "|    1|   1|       76.0357765|         80|5.1269595556932455|         8|1.6449| 23.85307565289762| 99.88885215289761| 2013-01-14|\n",
            "|    1|   1|76.11589099999999|         76| 5.273473599964619|         8|1.6449|24.534729280989193|100.65062028098919| 2013-01-15|\n",
            "|    1|   1|        76.179723|         78| 4.947341758580103|         8|1.6449|23.017407484150525| 99.19713048415052| 2013-01-16|\n",
            "|    1|   1|       76.3240495|         80| 4.720774754816659|         8|1.6449|21.963309080893463| 98.28735858089347| 2013-01-17|\n",
            "|    1|   1|76.54758650000001|         72| 4.386125310350268|         8|1.6449| 20.40635930796364| 96.95394580796365| 2013-01-18|\n",
            "|    1|   1|       76.8485265|         79|4.3094580368566735|         8|1.6449|20.049666368439013| 96.89819286843903| 2013-01-19|\n",
            "|    1|   1|77.22164049999999|         73|3.2071349029490923|         8|1.6449|14.921130279669068| 92.14277077966906| 2013-01-20|\n",
            "+-----+----+-----------------+-----------+------------------+----------+------+------------------+------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16 subquery p performs calculations on aggregated data.\n",
        "result_df = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  p.store,\n",
        "  p.item,\n",
        "  p.cycle_start,\n",
        "  p.cycle_stock,\n",
        "  p.zscore * SQRT(p.cycle_days) * p.cycle_stddev as safety_stock,\n",
        "  p.cycle_stock + (p.zscore * SQRT(p.cycle_days) * p.cycle_stddev) as required_stock,\n",
        "  p.cycle_sales\n",
        "FROM (\n",
        "  SELECT\n",
        "    a.store,\n",
        "    a.item,\n",
        "    a.cycle_start,\n",
        "    DATEDIFF(a.cycle_end, a.cycle_start) + 1 as cycle_days,\n",
        "    1.6449 as zscore, -- z-score for 95% SLE\n",
        "    a.cycle_stock,\n",
        "    a.cycle_stddev,\n",
        "    a.cycle_sales\n",
        "  FROM (\n",
        "    SELECT\n",
        "      x.store,\n",
        "      x.item,\n",
        "      y.cycle_start,\n",
        "      y.cycle_end,\n",
        "      SUM(x.sales_pred_mean) as cycle_stock,\n",
        "      SUM(x.sales) as cycle_sales,\n",
        "      AVG(x.sales) as cycle_stddev\n",
        "    FROM forecasts x\n",
        "    INNER JOIN (\n",
        "      SELECT DISTINCT -- cycles\n",
        "        store,\n",
        "        item,\n",
        "        date_add(next_day(date, 'THU'), -7) as cycle_start,\n",
        "        date_add(next_day(date, 'WED'), -1) as cycle_end\n",
        "      FROM forecasts\n",
        "      ) y\n",
        "      ON x.store=y.store AND x.item=y.item AND x.date BETWEEN y.cycle_start AND y.cycle_end\n",
        "    WHERE year(y.cycle_start) >= 2016 AND year(y.cycle_end) <= 2017\n",
        "    GROUP BY\n",
        "      x.store,\n",
        "      x.item,\n",
        "      y.cycle_start,\n",
        "      y.cycle_end\n",
        "    ) a\n",
        "  ) p\n",
        "WHERE p.store = 1 AND p.item = 1\n",
        "ORDER BY store, item, cycle_start\n",
        "\"\"\")\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jma352d9YCqP",
        "outputId": "dac95a6d-03cc-458d-f63d-7b3c3a29be1e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-----------+------------------+------------------+------------------+-----------+\n",
            "|store|item|cycle_start|       cycle_stock|      safety_stock|    required_stock|cycle_sales|\n",
            "+-----+----+-----------+------------------+------------------+------------------+-----------+\n",
            "|    1|   1| 2016-01-07| 92.73539000000001| 57.75137471662471|150.48676471662472|         86|\n",
            "|    1|   1| 2016-01-07|        196.526977| 78.01245316191019|274.53943016191016|        171|\n",
            "|    1|   1| 2016-01-14|        194.509709| 88.04914304238987|282.55885204238984|        193|\n",
            "|    1|   1| 2016-01-14| 90.08315199999998| 51.03609858678462| 141.1192505867846|         76|\n",
            "|    1|   1| 2016-01-21| 90.93764200000001| 69.83887175033685|160.77651375033685|        104|\n",
            "|    1|   1| 2016-01-21|        199.951837| 96.26098021732777|296.21281721732777|        211|\n",
            "|    1|   1| 2016-01-28|          95.03491|62.452068007512764|157.48697800751276|         93|\n",
            "|    1|   1| 2016-01-28|208.04831900000002| 88.04914304238987| 296.0974620423899|        193|\n",
            "|    1|   1| 2016-02-04| 98.30896799999998|54.393736651704664|152.70270465170464|         81|\n",
            "|    1|   1| 2016-02-04|211.71525150000002| 84.85565080769179| 296.5709023076918|        186|\n",
            "|    1|   1| 2016-02-11|       211.0487455|102.19175151033849|313.24049701033846|        224|\n",
            "|    1|   1| 2016-02-11| 98.42218650000001| 59.76595755557673|158.18814405557674|         89|\n",
            "|    1|   1| 2016-02-18|       213.3746965| 91.24263527708794|304.61733177708794|        200|\n",
            "|    1|   1| 2016-02-18|        97.7992475| 71.18192697630488| 168.9811744763049|        106|\n",
            "|    1|   1| 2016-02-25|100.65051100000001| 56.40831949065669| 157.0588304906567|         84|\n",
            "|    1|   1| 2016-02-25|       224.4206415| 92.61127480624425| 317.0319163062442|        203|\n",
            "|    1|   1| 2016-03-03|241.71043950000004|100.82311198118218| 342.5335514811822|        221|\n",
            "|    1|   1| 2016-03-03|       107.9602485| 68.49581652436883|176.45606502436885|        102|\n",
            "|    1|   1| 2016-03-10|116.50622400000002| 67.82428891138483|184.33051291138486|        101|\n",
            "|    1|   1| 2016-03-10|257.69892300000004|115.42193362551623|373.12085662551624|        253|\n",
            "+-----+----+-----------+------------------+------------------+------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# buffered forecast\n",
        "# Register the \"forecasts\" table as a temporary view\n",
        "forecasts.createOrReplaceTempView(\"forecasts\")\n",
        "\n",
        "# Execute the SQL query\n",
        "result_df = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  q.store,\n",
        "  q.item,\n",
        "  SUM(CASE WHEN q.cycle_sales > q.cycle_stock THEN 1 ELSE 0 END) as cycles_above_forecast,\n",
        "  SUM(CASE WHEN q.cycle_sales > q.required_stock THEN 1 ELSE 0 END) as cycles_above_buffered_forecast,\n",
        "  ROUND(100.0 * SUM(CASE WHEN q.cycle_sales > q.cycle_stock THEN 1 ELSE 0 END) / COUNT(*), 1) as cycles_above_forecast_pct,\n",
        "  ROUND(100.0 * SUM(CASE WHEN q.cycle_sales > q.required_stock THEN 1 ELSE 0 END) / COUNT(*), 1) as cycles_above_buffered_forecast_pct,\n",
        "  ROUND(100.0 * (1.0 - SUM(CASE WHEN q.cycle_sales > q.required_stock THEN 1 ELSE 0 END) / COUNT(*)), 1) as service_level\n",
        "FROM (\n",
        "  SELECT\n",
        "    p.store,\n",
        "    p.item,\n",
        "    p.cycle_start,\n",
        "    p.cycle_stock,\n",
        "    p.zscore * SQRT(p.cycle_days) * p.cycle_stddev as safety_stock,\n",
        "    p.cycle_stock + (p.zscore * SQRT(p.cycle_days) * p.cycle_stddev) as required_stock,\n",
        "    p.cycle_sales\n",
        "  FROM (\n",
        "    SELECT\n",
        "      a.store,\n",
        "      a.item,\n",
        "      a.cycle_start,\n",
        "      DATEDIFF(a.cycle_end, a.cycle_start)+1 as cycle_days,\n",
        "      1.6449 as zscore, -- z-score for 95% SLE\n",
        "      a.cycle_stock,\n",
        "      a.cycle_stddev,\n",
        "      a.cycle_sales\n",
        "    FROM (\n",
        "      SELECT\n",
        "        x.store,\n",
        "        x.item,\n",
        "        y.cycle_start,\n",
        "        y.cycle_end,\n",
        "        SUM(x.sales_pred_mean) as cycle_stock,\n",
        "        SUM(x.sales) as cycle_sales,\n",
        "        STDDEV(x.sales) as cycle_stddev\n",
        "      FROM forecasts x\n",
        "      INNER JOIN (\n",
        "        SELECT DISTINCT -- cycles\n",
        "          store,\n",
        "          item,\n",
        "          next_day(date_add(date, -7), 'thursday') as cycle_start,\n",
        "          next_day(date_add(date, -1), 'wednesday') as cycle_end\n",
        "        FROM forecasts\n",
        "      ) y\n",
        "      ON x.store=y.store AND x.item=y.item AND x.date BETWEEN y.cycle_start AND y.cycle_end\n",
        "      WHERE YEAR(y.cycle_start) >= 2013 AND YEAR(y.cycle_end) <= 2017\n",
        "      GROUP BY\n",
        "        x.store,\n",
        "        x.item,\n",
        "        y.cycle_start,\n",
        "        y.cycle_end\n",
        "    ) a\n",
        "  ) p\n",
        ") q\n",
        "GROUP BY q.store, q.item\n",
        "ORDER BY q.store, q.item\n",
        "\"\"\")\n",
        "\n",
        "# Show the result\n",
        "result_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owtPuUCsaHMQ",
        "outputId": "930c3b16-fe2d-40d8-f1f8-101e99c7b4ad"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+---------------------+------------------------------+-------------------------+----------------------------------+-------------+\n",
            "|store|item|cycles_above_forecast|cycles_above_buffered_forecast|cycles_above_forecast_pct|cycles_above_buffered_forecast_pct|service_level|\n",
            "+-----+----+---------------------+------------------------------+-------------------------+----------------------------------+-------------+\n",
            "|    1|   1|                  134|                            15|                     51.5|                               5.8|         94.2|\n",
            "|    2|   1|                  127|                            11|                     48.8|                               4.2|         95.8|\n",
            "|    3|   1|                  122|                            14|                     46.9|                               5.4|         94.6|\n",
            "|    4|   1|                  128|                             8|                     49.2|                               3.1|         96.9|\n",
            "|    5|   1|                  123|                            15|                     47.3|                               5.8|         94.2|\n",
            "|    6|   1|                   57|                             6|                     44.2|                               4.7|         95.3|\n",
            "+-----+----+---------------------+------------------------------+-------------------------+----------------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6RSiNoPKKEI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ew1g-lLPSFAC"
      }
    }
  ]
}